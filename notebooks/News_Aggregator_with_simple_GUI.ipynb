{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from scipy.spatial.distance import cosine, cityblock, jaccard, canberra, euclidean, minkowski, braycurtis\n",
    "stop_words = stopwords.words('english')\n",
    "from pyemd import emd\n",
    "from scipy.cluster.hierarchy import fclusterdata\n",
    "from sklearn.cluster import DBSCAN, AffinityPropagation, AgglomerativeClustering\n",
    "from sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier, kneighbors_graph\n",
    "import pickle\n",
    "import time\n",
    "import hashlib\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "#from itertools import combinations\n",
    "#from tqdm import tqdm_notebook\n",
    "#from scipy.stats import skew, kurtosis\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "#from sklearn.decomposition import PCA, TruncatedSVD, LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bbc_rss = ['http://feeds.bbci.co.uk/news/rss.xml', \n",
    "           'http://feeds.bbci.co.uk/news/world/rss.xml', \n",
    "           'http://feeds.bbci.co.uk/news/uk/rss.xml', \n",
    "           'http://feeds.bbci.co.uk/news/business/rss.xml', \n",
    "           'http://feeds.bbci.co.uk/news/politics/rss.xml', \n",
    "           'http://feeds.bbci.co.uk/news/health/rss.xml', \n",
    "           'http://feeds.bbci.co.uk/news/education/rss.xml', \n",
    "           'http://feeds.bbci.co.uk/news/science_and_environment/rss.xml', \n",
    "           'http://feeds.bbci.co.uk/news/technology/rss.xml', \n",
    "           'http://feeds.bbci.co.uk/news/entertainment_and_arts/rss.xml']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings:\n",
    "    \n",
    "    def __init__(self, create=False, ser_model_path='W2VModel',\n",
    "                 embeddings='GoogleNews-vectors-negative300.bin.gz',\n",
    "                 model_fun=gensim.models.KeyedVectors.load_word2vec_format, binary=True, norm=True):\n",
    "        self.ser_model = ser_model_path\n",
    "        self.embeddings = embeddings\n",
    "        self.model_fun = model_fun\n",
    "        self.binary = binary\n",
    "        self.norm = norm\n",
    "        \n",
    "        if create == False:\n",
    "            self.model = self.load_model()\n",
    "        else:\n",
    "            self.model = self.create_model()\n",
    "        \n",
    "            \n",
    "    def create_model(self):\n",
    "        model = self.model_fun(self.embeddings, binary=self.binary)\n",
    "        if self.norm:\n",
    "            model.init_sims(replace=True)\n",
    "        return model\n",
    "            \n",
    "    def load_model(self):\n",
    "        with open(self.ser_model, 'rb') as file:\n",
    "            model = pickle.load(file)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ? TODO: dist_matrix(x,y) from two news vectors (if we want some classification based only on distances, + use wmd!)\n",
    "class News_Vectorizer:\n",
    "    \n",
    "    def __init__(self, model, news=None):\n",
    "        self.news = news #array of strings\n",
    "        self.model = model #Word2Vec model\n",
    "        if self.news is not None:\n",
    "            self.news_vectors = self.news2vec(self.news) #vector representations\n",
    "        else:\n",
    "            self.news_vectors = None\n",
    "        self.cos_dist = None #cosine distance matrix\n",
    "        self.wm_dist = None #wmd-matrix\n",
    "    \n",
    "    def wmd(self, q1, q2):\n",
    "        q1 = str(q1).lower().split()\n",
    "        q2 = str(q2).lower().split()\n",
    "        q1 = [w for w in q1 if w not in stop_words]\n",
    "        q2 = [w for w in q2 if w not in stop_words]\n",
    "        return self.model.wmdistance(q1, q2)\n",
    "    \n",
    "    def sent2vec(self, s):\n",
    "        words = str(s).lower()\n",
    "        words = word_tokenize(words)\n",
    "        words = [w for w in words if not w in stop_words]\n",
    "        words = [w for w in words if w.isalpha()]\n",
    "        M = []\n",
    "        for w in words:\n",
    "            try:\n",
    "                M.append(self.model[w])\n",
    "            except:\n",
    "                continue\n",
    "        M = np.array(M)\n",
    "        v = M.sum(axis=0)\n",
    "        return v / np.sqrt((v ** 2).sum())\n",
    "    \n",
    "    def news2vec(self, news):\n",
    "        # update self.news, self.news_vectors\n",
    "        news_vectors = np.array([self.sent2vec(text) for text in news])\n",
    "        self.news = news\n",
    "        self.news_vectors = news_vectors\n",
    "        return news_vectors\n",
    "    \n",
    "    def dist_vec(self, news_item, news=None, metric='cosine'):\n",
    "        #computes distances between given item and news (or self.news)\n",
    "        news_item = self.sent2vec(news_item)\n",
    "        if news is not None:\n",
    "            news = self.news2vec(news)\n",
    "        else:\n",
    "            news = self.news_vectors\n",
    "        if news is None:\n",
    "            return 'no news to compute distances'\n",
    "        if metric == 'cosine':\n",
    "            dist_vec = np.array([cosine(news_item, i) for i in news])\n",
    "        elif metric == 'wmd':\n",
    "            dist_vec = np.array([self.wmd(news_item, i) for i in news])\n",
    "        return dist_vec\n",
    "    \n",
    "    def cosine_matrix(self): \n",
    "        cdist = np.zeros((len(self.news_vectors), len(self.news_vectors)))\n",
    "        for n, i in enumerate(self.news_vectors):\n",
    "            for m, j in enumerate(self.news_vectors):\n",
    "                cdist[n, m] = cosine(i, j)\n",
    "        self.cos_dist = cdist\n",
    "        return cdist\n",
    "    \n",
    "    def wmd_matrix(self): #list (news)\n",
    "        wmdist = np.zeros((len(self.news), len(self.news)))\n",
    "        for n, i in enumerate(self.news):\n",
    "            for m, j in enumerate(self.news):\n",
    "                wmdist[n, m] = self.wmd(i, j)\n",
    "        self.wm_dist = wmdist\n",
    "        return wmdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSS_Feeds:\n",
    "    \n",
    "    def __init__(self, urls):\n",
    "        self.urls = urls\n",
    "        self.feeds = self.get_feeds()\n",
    "        self.df_news = self.create_df()\n",
    "        self.df_unique_news = self.create_unique()\n",
    "        \n",
    "    def get_feeds(self):\n",
    "        return [feedparser.parse(feed) for feed in self.urls]\n",
    "    \n",
    "    def get_category(self, feed):\n",
    "        # sources may have different category names - agg categories?\n",
    "        return feed.feed.get('title', '')\n",
    "\n",
    "    def get_title_summary(self, feed, sep='. '): #get and join title and summary for each entry in feed\n",
    "        titles = [entry['title'] for entry in feed['entries']]\n",
    "        summaries = [entry['summary'] for entry in feed['entries']]\n",
    "        title_summary = [entry['title'] + sep + entry['summary'] for entry in feed['entries']]\n",
    "        return titles, summaries, title_summary\n",
    "    \n",
    "    def get_date(self, feed): #(year, month, day) for each entry in feed\n",
    "        return([entry['published_parsed'][:3] for entry in feed['entries']])\n",
    "    \n",
    "    def get_time(self, feed): #(hour, min, sec) for each entry in feed\n",
    "        return([entry['published_parsed'][3:6] for entry in feed['entries']])\n",
    "    \n",
    "    def get_datetime_nparsed(self, feed): #not parsed date and time for each entry in feed\n",
    "        return([entry['published'] for entry in feed['entries']])\n",
    "    \n",
    "    def get_link(self, feed): # link for each entry in feed\n",
    "        return([entry['link'] for entry in feed['entries']])\n",
    "    \n",
    "    def str2hash(self, s):\n",
    "        return hashlib.md5(s.encode()).hexdigest()\n",
    "    \n",
    "    def create_df(self): \n",
    "        news, title, summary, category, pdate, ptime, fdatetime, links  = [], [], [], [], [], [], [], []\n",
    "        for feed in self.feeds:\n",
    "            cat = self.get_category(feed)\n",
    "            titles, summaries, texts = self.get_title_summary(feed)\n",
    "            d_ymd, t_hms = self.get_date(feed), self.get_time(feed)\n",
    "            fdt = self.get_datetime_nparsed(feed)\n",
    "            news_links = self.get_link(feed)\n",
    "            \n",
    "            cat = np.resize([cat], len(texts))\n",
    "            news.extend(texts)\n",
    "            title.extend(titles)\n",
    "            summary.extend(summaries)\n",
    "            pdate.extend(d_ymd)\n",
    "            ptime.extend(t_hms)\n",
    "            fdatetime.extend(fdt)\n",
    "            links.extend(news_links)\n",
    "            category.extend(cat)\n",
    "        df_news = pd.DataFrame({'news':news, \n",
    "                                'category':category,\n",
    "                                'title':title, \n",
    "                                'summary':summary,\n",
    "                                'link':links,\n",
    "                                'date':pdate, \n",
    "                                'time':ptime, \n",
    "                                'datetime':fdatetime})\n",
    "        df_news['ID'] = df_news.news.apply(self.str2hash)\n",
    "        self.df_news = df_news\n",
    "        return df_news\n",
    "    \n",
    "    def create_unique(self):\n",
    "        df_unique_news = self.df_news.groupby('news').agg({'category':list, \n",
    "                                                           'title': np.unique, \n",
    "                                                           'summary': np.unique, \n",
    "                                                           'link': np.unique, \n",
    "                                                           'date': np.unique, \n",
    "                                                           'time': np.unique, \n",
    "                                                           'datetime': np.unique, \n",
    "                                                           'ID': np.unique})\n",
    "        df_unique_news.reset_index(inplace=True)\n",
    "        self.df_unique_news = df_unique_news\n",
    "        return df_unique_news\n",
    "    \n",
    "    def get_unique_news(self):\n",
    "        return self.df_unique_news.news.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Aggregator:\n",
    "    \n",
    "    def __init__(self, clusterizer, classifier, labeled_data=None, labels=None, clust_weights=None):\n",
    "        self.clusterizer = clusterizer\n",
    "        self.classifier = classifier\n",
    "        self.labeled_data = labeled_data #already clustered viewed, vector representations as ndarray\n",
    "        self.labels = labels #clust nums of labeled_data, ndarray\n",
    "        self.clust_weights = clust_weights # DataFrame, colnames=['clust', 'weight']\n",
    "        \n",
    "    def clusterize(self, data):\n",
    "        labels = self.clusterizer.fit_predict(data)\n",
    "        return data, labels\n",
    "    \n",
    "    def classify(self, new_data): #if one sample: reshape sent2vec output to (1, 300)\n",
    "        try:\n",
    "            predicted = self.classifier.predict(new_data)\n",
    "        except NotFittedError as e:\n",
    "            return(repr(e))\n",
    "        return predicted\n",
    "    \n",
    "    def fit_classifier(self):\n",
    "        X, y = self.labeled_data, self.labels\n",
    "        self.classifier.fit(X, y)\n",
    "        return self.classifier\n",
    "    \n",
    "    def prep_data(self, new_data=None):\n",
    "        if self.labeled_data is None and new_data is None:\n",
    "            return None\n",
    "        else:\n",
    "            try:\n",
    "                ldata = pd.DataFrame(self.labeled_data)\n",
    "            except:\n",
    "                ldata = None\n",
    "            try:\n",
    "                ndata = pd.DataFrame(new_data)\n",
    "            except:\n",
    "                ndata=None\n",
    "            try:\n",
    "                data = pd.concat([ldata, ndata]).values\n",
    "                return data\n",
    "            except:\n",
    "                return None\n",
    "    \n",
    "    def update_weights(self): #sum weights = 1 required in News_Finder\n",
    "        unique, counts = np.unique(self.labels[self.labels != -1], return_counts=True)\n",
    "        weights = counts/counts.sum() #smth like this\n",
    "        weights = np.asarray((unique, weights)).T # [label, weight]\n",
    "        self.clust_weights = pd.DataFrame({'clust': weights[:,0].astype(int), 'weight': weights[:,1]})\n",
    "        return self.clust_weights\n",
    "    \n",
    "    def update_aggregator(self, new_data):\n",
    "        data = self.prep_data(new_data=new_data)\n",
    "        if data is None:\n",
    "            return 'no data'\n",
    "        else:\n",
    "            self.labeled_data, self.labels = self.clusterize(data)    \n",
    "            self.fit_classifier()\n",
    "            self.update_weights()\n",
    "            return 'updated'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class News_Finder():\n",
    "    \n",
    "    def __init__(self, df_news, news_vectorizer): #df_news: DF with non-viewed news items; News_Vectorizer instance\n",
    "        self.df_news = df_news\n",
    "        self.df_unique_news = self.create_unique()\n",
    "        self.news_vectorizer = news_vectorizer\n",
    "        \n",
    "    def update_news(self, df_news):\n",
    "        self.df_news = df_news\n",
    "        self.df_unique_news = self.create_unique()\n",
    "        return 'updated'\n",
    "    \n",
    "    def create_unique(self):\n",
    "        df_unique_news = self.df_news.groupby('ID').agg({'news': np.unique, \n",
    "                                                         'category':list, \n",
    "                                                         'title': np.unique, \n",
    "                                                         'summary': np.unique, \n",
    "                                                         'link': np.unique, \n",
    "                                                         'date': np.unique, \n",
    "                                                         'time': np.unique, \n",
    "                                                         'datetime': np.unique})\n",
    "        df_unique_news.reset_index(inplace=True)\n",
    "        return df_unique_news\n",
    "    \n",
    "    def get_from_categories(self, n=5):\n",
    "        # returns n top news from each category\n",
    "        return self.df_news.groupby('category').head(n) \n",
    "    \n",
    "    def get_similar(self, news_item_ID, metric='cosine', n=5):\n",
    "        # returns the n most similar news to news_item\n",
    "        all_news = self.df_unique_news.query('ID != @news_item_ID').copy()\n",
    "        news_item = self.df_unique_news.query('ID == @news_item_ID').copy()\n",
    "        dist_vec = self.news_vectorizer.dist_vec(news_item.news, all_news.news.values, metric=metric)\n",
    "        all_news['dist'] = dist_vec\n",
    "        return all_news.nsmallest(n, 'dist')\n",
    "    \n",
    "    def get_interesting(self, aggregator, n=20): #fitted Aggregator instance for classification & weights\n",
    "        # TODO: return n news\n",
    "        # TODO: if there are not enough news in clusters (for weights), return another news?\n",
    "        all_news = self.df_unique_news.copy()\n",
    "        news_vec = self.news_vectorizer.news2vec(all_news.news.values)\n",
    "        #print(news_vec.shape)\n",
    "        weights = aggregator.clust_weights\n",
    "        labels = aggregator.classify(news_vec)\n",
    "        all_news['label'] = labels\n",
    "        all_news = all_news.query('label != -1')\n",
    "        # TODO: filter 'outlier' cluster\n",
    "        # ?TODO: distances to choose the most relevant items in cluster \n",
    "        n_from_cluster = np.ceil((aggregator.clust_weights.weight*n)).astype(int)\n",
    "        dflist = []\n",
    "        for cluster, n in zip(weights.clust, n_from_cluster):\n",
    "            dflist.append(all_news.query('label == @cluster').head(n))\n",
    "        interesting = pd.concat(dflist)\n",
    "        return interesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_Manager:\n",
    "    \n",
    "    def __init__(self, path_dict=None): #path_dict {'csv':{obj:path}, 'serialized':{obj:path}}\n",
    "        self.path_dict = path_dict\n",
    "        if self.path_dict is not None:\n",
    "            self.data_dict = self.load_data()\n",
    "        else:\n",
    "            self.data_dict = {}\n",
    "        \n",
    "    def load_data(self):\n",
    "        data_dict = {}\n",
    "        try:\n",
    "            for obj_name, path in self.path_dict['csv'].items():\n",
    "                data_dict[obj_name] = pd.read_csv(path, index_col=0)\n",
    "        except:\n",
    "            print('something is not ok with \"csv\" key or it does not exist')\n",
    "        try:\n",
    "            for obj_name, path in self.path_dict['serialized'].items():\n",
    "                with open(path, 'rb') as file:\n",
    "                    data_dict[obj_name] = pickle.load(file)\n",
    "        except:\n",
    "            print('something is not ok with \"serialized\" key or it does not exist')\n",
    "        return data_dict\n",
    "    \n",
    "    def delete_old(self, obj_name, n_recent=100):\n",
    "        #del old data, except n_recent\n",
    "        data = self.get_data_item(obj_name)\n",
    "        if type(data) == pd.core.frame.DataFrame and data.shape[0] > n_recent:\n",
    "            data = data.tail(n_recent)\n",
    "            self.update_data_item(obj_name, data, concat=False)\n",
    "            return('old entries removed')\n",
    "        return('not enough entries to delete or is not DF')\n",
    "    \n",
    "    #def prep_data(self):\n",
    "    #    #maybe some data manipulations\n",
    "    #    pass\n",
    "    \n",
    "    def get_data_item(self, obj_name):\n",
    "        return self.data_dict.get(obj_name, 'Does not exist')\n",
    "    \n",
    "    def update_data_item(self, obj_name, new_data, concat=False): #concat [True, False] - if concat data\n",
    "        if concat == False:\n",
    "            self.data_dict[obj_name] = new_data\n",
    "            return('upd: set data_dict[obj] = new_data')\n",
    "        elif concat == True:\n",
    "            if obj_name in self.data_dict.keys():\n",
    "                data = self.get_data_item(obj_name)\n",
    "                if type(data) == pd.core.frame.DataFrame:\n",
    "                    try:\n",
    "                        data = pd.concat([data, new_data], sort=False)\n",
    "                        self.data_dict[obj_name] = data\n",
    "                        return 'updated'\n",
    "                    except:\n",
    "                        return 'could not update'\n",
    "                elif type(data) == np.ndarray:\n",
    "                    try:\n",
    "                        data = np.vstack([data, new_data])\n",
    "                        self.data_dict[obj_name] = data\n",
    "                        return 'updated'\n",
    "                    except:\n",
    "                        return 'could not update'\n",
    "                else:\n",
    "                    self.data_dict[obj_name] = new_data\n",
    "                    return 'upd: obj = new_data (not an array or DF)'\n",
    "            else:\n",
    "                self.data_dict[obj_name] = new_data\n",
    "                return 'upd: obj = new_data (obj did not exist yet)'\n",
    "    \n",
    "    def save_model(self, data_items='all'): #data_items: 'all' or list of keys for data_dict\n",
    "        if data_items == 'all':\n",
    "            data_items = self.data_dict.keys()\n",
    "        for obj_name in data_items:\n",
    "            data = self.data_dict[obj_name]\n",
    "            if type(data) == pd.core.frame.DataFrame:\n",
    "                data.to_csv(obj_name + '.csv')\n",
    "            elif type(data) == np.ndarray:\n",
    "                # are there any ndarrays?..\n",
    "                # TODO: write csv...\n",
    "                pass\n",
    "            else:\n",
    "                with open(obj_name, 'wb') as file:\n",
    "                    pickle.dump(data, file)\n",
    "        return 'saved'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schnittstelle-Klasse zu GUI\n",
    "\n",
    "(volle Funktionalität, erfordert alle oben definierten Klassen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Communicator:\n",
    "    '''Application start -> initialize Communicator instance and call `start()`\n",
    "    to start an existing system or create a new system and get DataFrames, e.g.:\n",
    "    \n",
    "    `CI = Communicator()\n",
    "    news_in_categories, interesting_news = CI.start()\n",
    "    display(news_in_categories)`\n",
    "    \n",
    "    Then use `handle_input()` to process user input and get system output, e.g.:\n",
    "    \n",
    "    `news_in_categories, interesting_news = CI.handle_input(u_input='upd')` '''   \n",
    "    \n",
    "    def __init__(self, feeds='default', model='default', path_dict='default', data_items_names='default', \n",
    "                 n_from_cats=10, n_interesting=20, n_similar=5, THRESHOLD=20):\n",
    "        \n",
    "        if feeds == 'default':\n",
    "            self.feeds = ['http://feeds.bbci.co.uk/news/rss.xml', \n",
    "                          'http://feeds.bbci.co.uk/news/world/rss.xml', \n",
    "                          'http://feeds.bbci.co.uk/news/uk/rss.xml', \n",
    "                          'http://feeds.bbci.co.uk/news/business/rss.xml', \n",
    "                          'http://feeds.bbci.co.uk/news/politics/rss.xml', \n",
    "                          'http://feeds.bbci.co.uk/news/health/rss.xml', \n",
    "                          'http://feeds.bbci.co.uk/news/education/rss.xml', \n",
    "                          'http://feeds.bbci.co.uk/news/science_and_environment/rss.xml', \n",
    "                          'http://feeds.bbci.co.uk/news/technology/rss.xml', \n",
    "                          'http://feeds.bbci.co.uk/news/entertainment_and_arts/rss.xml']\n",
    "        else:\n",
    "            self.feeds = feeds\n",
    "            \n",
    "        if model == 'default':\n",
    "            self.model = Embeddings().model\n",
    "        else:\n",
    "            self.model = model\n",
    "        \n",
    "        if path_dict == 'default':\n",
    "            self.path_dict = {'csv':{'df_viewed':'df_viewed.csv', \n",
    "                                     'df_labeled':'df_labeled.csv', \n",
    "                                     'clust_weights':'clust_weights.csv'}, \n",
    "                              'serialized':{'classifier':'classifier', \n",
    "                                            'clusterizer':'clusterizer'}}\n",
    "        else:\n",
    "            self.path_dict = path_dict\n",
    "            \n",
    "        if data_items_names == 'default':\n",
    "            self.data_items_names = ['df_viewed', 'df_labeled', 'clust_weights', 'classifier', 'clusterizer']\n",
    "        else:\n",
    "            self.data_items_names = data_items_names\n",
    "        \n",
    "        self.STATE = None\n",
    "        self.n_from_cats = n_from_cats\n",
    "        self.n_interesting = n_interesting\n",
    "        self.n_similar = n_similar\n",
    "        self.THRESHOLD = THRESHOLD\n",
    "        \n",
    "        self.DM = None\n",
    "        self.NFind = None\n",
    "        self.AGG = None\n",
    "        \n",
    "        self.RSS = RSS_Feeds(self.feeds)\n",
    "        self.NVec = News_Vectorizer(model=self.model)\n",
    "        \n",
    "        self.init_classification_model = RadiusNeighborsClassifier(radius=0.49, weights='distance', \n",
    "                                                                   metric='cosine', outlier_label='-1')\n",
    "        self.init_clustering_model = AgglomerativeClustering(n_clusters=None, affinity='cosine', \n",
    "                                                             linkage='complete', distance_threshold=0.65)\n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "    def create_new_system(self):\n",
    "        # create a new system without any already existing data\n",
    "        self.DM = Data_Manager()\n",
    "        self.AGG = Aggregator(self.init_clustering_model, self.init_classification_model)\n",
    "        \n",
    "        self.DM.update_data_item('df_news', self.RSS.df_news, concat=False)\n",
    "        self.DM.update_data_item('df_viewed', pd.DataFrame(columns=self.RSS.df_news.columns), concat=False) # DF for viewed news\n",
    "        self.DM.update_data_item('classifier', self.AGG.classifier, concat=False)\n",
    "        self.DM.update_data_item('clusterizer', self.AGG.clusterizer, concat=False)\n",
    "        return True\n",
    "    \n",
    "    def load_data_models(self):\n",
    "        # try to load data and check if all data items in data_items_names in data_dict\n",
    "        # set STATE ('ALL' - already fitted model, all data; 'NOT FITTED' - not fitted yet, some data; 'NEW' - no data)\n",
    "\n",
    "        self.DM = Data_Manager(path_dict=self.path_dict)\n",
    "        if len([i for i in self.data_items_names if i not in self.DM.data_dict.keys()]) == 0:\n",
    "            return ('ALL')\n",
    "        elif ('df_viewed' in self.DM.data_dict.keys()) and ('clust_weights' not in self.DM.data_dict.keys()):\n",
    "            return 'NOT FITTED'\n",
    "        else:\n",
    "            return 'NEW'\n",
    "        \n",
    "    def select_news(self):\n",
    "        categories = self.NFind.get_from_categories(n=self.n_from_cats)\n",
    "        if self.STATE == 'ALL':\n",
    "            interesting = self.NFind.get_interesting(self.AGG, n=self.n_interesting)\n",
    "        else:\n",
    "            interesting = self.NFind.df_unique_news.sample(self.n_interesting)\n",
    "        return categories, interesting\n",
    "    \n",
    "    def check_viewed(self):\n",
    "        # check and remove already viewed news from news DataFrame\n",
    "        if self.STATE == 'ALL':\n",
    "            viewed_id = np.hstack([self.DM.get_data_item('df_labeled').ID.values, self.DM.get_data_item('df_viewed').ID.values])\n",
    "        elif self.STATE == 'NOT FITTED':\n",
    "            viewed_id = self.DM.get_data_item('df_viewed').ID.values\n",
    "        self.DM.update_data_item('df_news', self.RSS.df_news.query('ID not in @viewed_id'), concat=False)\n",
    "        return True\n",
    "    \n",
    "    def start(self):\n",
    "        #try to load data\n",
    "        #if 'ALL' -> start existing\n",
    "        #if 'NEW' -> call `create_new_system` and change STATE\n",
    "        #if 'NOT FITTED' -> init aggregator with existing models\n",
    "        # return DataFrames\n",
    "        \n",
    "        self.STATE = self.load_data_models()\n",
    "        \n",
    "        if self.STATE == 'NOT FITTED':\n",
    "            self.AGG = Aggregator(clusterizer=self.DM.get_data_item('clusterizer'), \n",
    "                                  classifier=self.DM.get_data_item('classifier'))\n",
    "            # filter already viewed news\n",
    "            self.check_viewed()          \n",
    "            \n",
    "        elif self.STATE == 'ALL':\n",
    "            self.DM.delete_old('df_labeled')    \n",
    "            self.AGG = Aggregator(clusterizer=self.DM.get_data_item('clusterizer'), \n",
    "                                  classifier=self.DM.get_data_item('classifier'), \n",
    "                                  labeled_data=self.NVec.news2vec(self.DM.get_data_item('df_labeled').news.values), \n",
    "                                  labels=self.DM.get_data_item('df_labeled').label.values, \n",
    "                                  clust_weights=self.DM.get_data_item('clust_weights'))\n",
    "            # filter already viewed news\n",
    "            self.check_viewed()\n",
    "        \n",
    "        elif self.STATE == 'NEW':\n",
    "            self.create_new_system()\n",
    "            self.STATE = 'NOT FITTED'\n",
    "        \n",
    "        #initialize News_Finder\n",
    "        self.NFind = News_Finder(self.DM.get_data_item('df_news'), News_Vectorizer(model=self.model))\n",
    "        \n",
    "        # get news DataFrames to show\n",
    "        news_from_categories, news_interesting = self.select_news()\n",
    "            \n",
    "        return news_from_categories, news_interesting\n",
    "    \n",
    "\n",
    "    def handle_input(self, u_input): # the main method that GUI has to call\n",
    "        ''' Call this method with `u_input` argument to communicate with the system.\n",
    "            It takes a string `u_input` and returns appropriate output.\n",
    "            \n",
    "            Interactions as `u_input` -> `method output`:\n",
    "            \n",
    "            * 'upd' -> two pandas DataFrame objects (news in categories, interesting news)\n",
    "            * 'exit' -> boolean: True, if data and models have been successfully saved, False otherwise\n",
    "            * 'viewed' + ' ' + ID (e.g. 'viewed 005503512f38f130303cb133d656203b') -> two pandas DataFrame objects (news in categories, interesting news)\n",
    "            * 'similar' + ' ' + ID (e.g. 'similar dc313bbf1bfca18d28e95862e972822f') -> pandas DataFrame with nearest news            \n",
    "        '''\n",
    "        # parse GUI input\n",
    "        # call appropriate methods\n",
    "        # return system output to GUI\n",
    "        instruction = u_input.split()[0]\n",
    "        if instruction == 'upd':\n",
    "            #update news\n",
    "            categories, interesting = self.update_news()\n",
    "            return categories, interesting\n",
    "        \n",
    "        elif instruction == 'exit':\n",
    "            is_saved = self.save()\n",
    "            return is_saved\n",
    "        \n",
    "        elif instruction == 'viewed':\n",
    "            #get news id and process viewed...\n",
    "            n_id = u_input.split()[1:] # list, even if it consists of only one ID (generally)\n",
    "            categories, interesting = self.handle_viewed(n_id)\n",
    "            return categories, interesting\n",
    "\n",
    "        elif instruction == 'similar':\n",
    "            #get id and return similar\n",
    "            n_id = u_input.split()[1] # string\n",
    "            similar_news = self.find_similar(n_id)\n",
    "            return similar_news\n",
    "\n",
    "        else:\n",
    "            return('unknown input')\n",
    "        \n",
    "    \n",
    "    def update_news(self):\n",
    "        # download news from rss feeds\n",
    "        self.RSS = RSS_Feeds(self.feeds)\n",
    "        \n",
    "        # filter already viewed news\n",
    "        self.check_viewed()\n",
    "        \n",
    "        # update news in News_Finder\n",
    "        self.NFind.update_news(self.DM.get_data_item('df_news'))\n",
    "        # return DataFrames\n",
    "        news_from_categories, news_interesting = self.select_news()\n",
    "            \n",
    "        return news_from_categories, news_interesting\n",
    "    \n",
    "    def save(self):\n",
    "        try:\n",
    "            self.DM.save_model()\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def handle_viewed(self, n_id): #n_id : list\n",
    "        # get news ID, check if already in viewed, remove from all news,.....\n",
    "        viewed = self.NFind.df_unique_news.query('ID in @n_id')\n",
    "        self.NFind.update_news(self.NFind.df_news.query('ID not in @n_id')) #filter already viewed and update data\n",
    "        self.DM.update_data_item('df_viewed', viewed, concat=True)\n",
    "        self.DM.update_data_item('df_news', self.NFind.df_news, concat=False)\n",
    "\n",
    "        # if n(viewed) >= THRESHOLD -> update model -> save model\n",
    "        if self.DM.get_data_item('df_viewed').shape[0] >= self.THRESHOLD:\n",
    "            #update aggregator\n",
    "            data = self.DM.get_data_item('df_viewed')\n",
    "            colnames = data.columns\n",
    "            self.AGG.update_aggregator(self.NVec.news2vec(data.news))\n",
    "            \n",
    "            #update df_labeled\n",
    "            if self.STATE == 'ALL':\n",
    "                data = pd.concat([self.DM.get_data_item('df_labeled').drop('label', axis=1), data])\n",
    "            data['label'] = self.AGG.labels\n",
    "            self.DM.update_data_item('df_labeled', data, concat=False)\n",
    "            \n",
    "            #update df_viewed (empty)\n",
    "            self.DM.update_data_item('df_viewed', pd.DataFrame(columns=colnames), concat=False)\n",
    "            \n",
    "            #update data_items: classifier, clusterizer etc\n",
    "            self.DM.update_data_item('classifier', self.AGG.classifier, concat=False)\n",
    "            self.DM.update_data_item('clusterizer', self.AGG.clusterizer, concat=False)\n",
    "            self.DM.update_data_item('clust_weights', self.AGG.clust_weights, concat=False)\n",
    "            \n",
    "            # update STATE\n",
    "            self.STATE = 'ALL'\n",
    "            \n",
    "            #save model\n",
    "            self.save()\n",
    "            \n",
    "        # return DataFrames\n",
    "        news_from_categories, news_interesting = self.select_news()\n",
    "        return news_from_categories, news_interesting\n",
    "    \n",
    "    \n",
    "    def find_similar(self, n_id): #n_id : str\n",
    "        # return the n nearest news to the given news item\n",
    "        return self.NFind.get_similar(news_item_ID=n_id, n=self.n_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from IPython.core.display import display, HTML, clear_output\n",
    "import webbrowser\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_item(value, placeholder='', description=''):\n",
    "    w = widgets.HTML(\n",
    "            value=value,\n",
    "            placeholder=placeholder,\n",
    "            description=description\n",
    "        )\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_similar(df):\n",
    "    clear_output()\n",
    "    display(widgets.HBox([UpdButton(), CloseButton()]))\n",
    "    litems = [create_item('<b>'+str(i[1].title)+'</b>  <i>(' + str(i[1].datetime) + ')</i><br>'+str(i[1].summary) + '<br>') for i in df.iterrows()]\n",
    "    ritems = [widgets.HBox([LinkButton(ID=i[1].ID, link=i[1].link), ShowSimilarButton(ID=i[1].ID)]) for i in df.iterrows()]\n",
    "    items = []\n",
    "    for i, j in zip(litems, ritems):\n",
    "        items.append(i)\n",
    "        items.append(j)\n",
    "    display(widgets.GridBox(items, layout=widgets.Layout(grid='none / repeat(2)')))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_update(df_categories, df_interest):\n",
    "    clear_output()\n",
    "    print(time.ctime())\n",
    "    display(widgets.HBox([UpdButton(), CloseButton()]))   \n",
    "    cat_title = create_item('<br><h2>Interesting</h2>')\n",
    "    litems = [create_item('<b>'+str(i[1].title)+'</b>  <i>(' + str(i[1].datetime) + ')</i><br>'+str(i[1].summary) + '<br>') for i in df_interest.iterrows()]\n",
    "    ritems = [widgets.HBox([LinkButton(ID=i[1].ID, link=i[1].link), ShowSimilarButton(ID=i[1].ID)]) for i in df_interest.iterrows()]\n",
    "    items = []\n",
    "    for i, j in zip(litems, ritems):\n",
    "        items.append(i)\n",
    "        items.append(j)\n",
    "    display(cat_title)\n",
    "    display(widgets.GridBox(items, layout=widgets.Layout(grid_template_columns=\"repeat(2\")))\n",
    "    #display(widgets.GridBox(items, layout=widgets.Layout(grid_template_columns=\"repeat(2, 700px)\")))\n",
    "    #display(widgets.GridBox(items, layout=widgets.Layout(grid='none / repeat(1)')))\n",
    "    \n",
    "    for cat in df_categories.category.unique():\n",
    "        df = df_categories.query('category == @cat')#.head(10)\n",
    "        litems = [create_item('<b>'+str(i[1].title)+'</b>  <i>(' + str(i[1].datetime) + ')</i><br>'+str(i[1].summary) + '<br>') for i in df.iterrows()]\n",
    "        ritems = [widgets.HBox([LinkButton(ID=i[1].ID, link=i[1].link), ShowSimilarButton(ID=i[1].ID)]) for i in df.iterrows()]\n",
    "        cat_title = create_item('<br><h2>' + str(cat) + '</h2>')\n",
    "        items = []\n",
    "        for i, j in zip(litems, ritems):\n",
    "            items.append(i)\n",
    "            items.append(j)\n",
    "        display(cat_title)\n",
    "        #display(widgets.GridBox(items, layout=widgets.Layout(grid_template_columns=\"repeat(2, 700px)\")))\n",
    "        display(widgets.GridBox(items, layout=widgets.Layout(grid='none / repeat(2)')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upd_button_clicked(b):\n",
    "    cat, interest = CI.handle_input('upd')\n",
    "    view_update(cat, interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkButton(widgets.Button):\n",
    "    \n",
    "    global CI\n",
    "    global view_update\n",
    "    \n",
    "    def __init__(self, ID, link='https://www.google.com/', description='Read', disabled=False, style='', tooltip='Link to article', icon='link', *args, **kwargs):\n",
    "        \"\"\"Initialize the LinkButton class.\"\"\"\n",
    "        super(LinkButton, self).__init__(*args, **kwargs)\n",
    "        # Create the button.\n",
    "        self.link = link\n",
    "        self.ID = ID\n",
    "        self.description = description\n",
    "        self.disabled = disabled\n",
    "        #self.style = style\n",
    "        self.tooltip = tooltip\n",
    "        self.icon = \"link\"\n",
    "        self.style.button_color = \"lightblue\"\n",
    "        # Set on click behavior.\n",
    "        self.on_click(self.process)\n",
    "\n",
    "    def process(self, b):\n",
    "        webbrowser.open(self.link)\n",
    "        cat, interest = CI.handle_input('viewed '+ self.ID)\n",
    "        view_update(cat, interest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpdButton(widgets.Button):\n",
    "    \n",
    "    global CI\n",
    "    global view_update\n",
    "    \n",
    "    def __init__(self, description='Update', disabled=False, style='', tooltip='Download news', icon='refresh', *args, **kwargs):\n",
    "        \"\"\"Initialize the LinkButton class.\"\"\"\n",
    "        super(UpdButton, self).__init__(*args, **kwargs)\n",
    "        # Create the button.\n",
    "        self.description = description\n",
    "        self.disabled = disabled\n",
    "        #self.style = style\n",
    "        self.tooltip = tooltip\n",
    "        self.icon = icon\n",
    "        self.style.button_color = \"lightgreen\"\n",
    "        # Set on click behavior.\n",
    "        self.on_click(self.process)\n",
    "\n",
    "    def process(self, b):\n",
    "        cat, interest = CI.handle_input('upd')\n",
    "        view_update(cat, interest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CloseButton(widgets.Button):\n",
    "    \n",
    "    global CI\n",
    "    #global view_update\n",
    "    \n",
    "    def __init__(self, description='Close', disabled=False, style='', tooltip='Exit', icon='check', *args, **kwargs):\n",
    "        \"\"\"Initialize the LinkButton class.\"\"\"\n",
    "        super(CloseButton, self).__init__(*args, **kwargs)\n",
    "        # Create the button.\n",
    "        self.description = description\n",
    "        self.disabled = disabled\n",
    "        #self.style = style\n",
    "        self.tooltip = tooltip\n",
    "        self.icon = icon\n",
    "        self.style.button_color = \"gray\"\n",
    "        # Set on click behavior.\n",
    "        self.on_click(self.process)\n",
    "\n",
    "    def process(self, b):\n",
    "        is_saved = CI.handle_input('exit')\n",
    "        clear_output()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShowSimilarButton(widgets.Button):\n",
    "    \n",
    "    global CI\n",
    "    global view_update\n",
    "    \n",
    "    def __init__(self, ID, description='Show similar', disabled=False, style='', tooltip='Find similar news', icon='search', *args, **kwargs):\n",
    "        \"\"\"Initialize the ShowSimilarButton class.\"\"\"\n",
    "        super(ShowSimilarButton, self).__init__(*args, **kwargs)\n",
    "        # Create the button.\n",
    "        self.ID = ID\n",
    "        self.description = description\n",
    "        self.disabled = disabled\n",
    "        #self.style = style\n",
    "        self.tooltip = tooltip\n",
    "        self.icon = icon\n",
    "        self.style.button_color = \"thistle\"\n",
    "        # Set on click behavior.\n",
    "        self.on_click(self.process)\n",
    "\n",
    "    def process(self, b):\n",
    "        similar = CI.handle_input('similar '+ self.ID)\n",
    "        view_similar(similar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "CI = Communicator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat, interest = CI.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan 23 15:22:09 2020\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b05396c20d74854b44a090f15fa128c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(UpdButton(description='Update', icon='refresh', style=ButtonStyle(button_color='lightgreen'), t…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92aca4c7e76a483cb99eba22ff8f0ebd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<br><h2>Interesting</h2>', placeholder='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2f55cc1944d4ec5bc9684777e87b320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridBox(children=(HTML(value='<b>Labour leadership: Thornberry pressed on \\'selective\\' school choice</b>  <i>…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54b286c0c55f446b830f0a1f3ece5d14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<br><h2>BBC News - Home</h2>', placeholder='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b97653860e34480796ba8b79b1aa3e24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridBox(children=(HTML(value='<b>Coronavirus: \\'Increased likelihood\\' of cases in the UK</b>  <i>(Thu, 23 Jan…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5127ff9463a94dfcbcd40c37f2114993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<br><h2>BBC News - World</h2>', placeholder='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc53a2b5b7ac4f68a00eea70f46c3b28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridBox(children=(HTML(value=\"<b>Meet Vyom - India's first robot 'astronaut'</b>  <i>(Thu, 23 Jan 2020 08:44:0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7471fc55adf47199abf8a3473cd7118",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<br><h2>BBC News - UK</h2>', placeholder='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e38e4a7ee3541a39a6c51a99021150e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridBox(children=(HTML(value='<b>Morrisons supermarket axes 3,000 managers in huge shake-up</b>  <i>(Thu, 23 J…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5c347659c92412bb7a5cccce63831c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<br><h2>BBC News - Business</h2>', placeholder='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b72b513a0044d49a784cf65c59f4d1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridBox(children=(HTML(value='<b>Morrisons supermarket axes 3,000 managers in huge shake-up</b>  <i>(Thu, 23 J…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b11b44f10674cbeb88a3789ef59cb58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<br><h2>BBC News - UK Politics</h2>', placeholder='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aff10001dbf415aa383830f498f6a3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridBox(children=(HTML(value='<b>Coronavirus: \\'Increased likelihood\\' of cases in the UK</b>  <i>(Thu, 23 Jan…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "889f9cdc4db347b7a0213bb2f1b6a760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<br><h2>BBC News - Health</h2>', placeholder='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b731bf98472c479b82a92cc9d81dd32e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridBox(children=(HTML(value='<b>Coronavirus: \\'Increased likelihood\\' of cases in the UK</b>  <i>(Thu, 23 Jan…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4466e7cd08804968afb2eedcc6f064f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<br><h2>BBC News - Family & Education</h2>', placeholder='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "954a54574ba64952a8c3a34b0c5116fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridBox(children=(HTML(value=\"<b>One in four children 'has too little sleep'</b>  <i>(Thu, 23 Jan 2020 01:07:1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b3d0e168dcb47359cf1fe2e3cea74d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<br><h2>BBC News - Science & Environment</h2>', placeholder='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd4a1bbd4ece4b25a696eedcf04fdde7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridBox(children=(HTML(value=\"<b>Wanted - volunteers to monitor Britain's growing slug population</b>  <i>(Thu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28a2c9cfdbee4c64ba18531bc2b1aed0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<br><h2>BBC News - Technology</h2>', placeholder='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f046a5f9e6344307aac6eda8387f91f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridBox(children=(HTML(value=\"<b>Twitter demands AI company stops 'collecting faces'</b>  <i>(Thu, 23 Jan 2020…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce0da0e60de04fdc905e7f7c6d3adf9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<br><h2>BBC News - Entertainment & Arts</h2>', placeholder='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c88fb3e217142c69c788c96a5c51949",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridBox(children=(HTML(value='<b>Jessica Simpson reveals childhood sexual abuse</b>  <i>(Thu, 23 Jan 2020 09:5…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "view_update(cat, interest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
