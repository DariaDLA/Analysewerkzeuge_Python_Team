{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Aggregator\n",
    "---\n",
    "<p style=\"text-align: right; font-size: 18px\"> <i>Daria Liakhovets   <br>   Katalin Feichtinger <br> Stefan Kostelecky  </i></p>\n",
    " \n",
    "---\n",
    "\n",
    "### **Ziel:** \n",
    "Eine funktionierende Software, die es ermöglicht, Nachrichten aus verschiedenen Quellen zu aggregieren. Dabei sollten die Nachrichten sinnvollen Kategorien zugeordnet werden. Das System sollte außerdem Präferenzen des Users berücksichtigen und die relevanten Artikel in einer speziellen personalisierten Kategorie anzeigen.\n",
    "\n",
    "#### Das Projekt benötigt folgende Komponenten:\n",
    "* *Daten* aus rss feeds\n",
    "* *Logik*, bestehend aus einfachem Sortieren von Artikeln nach Kategorien und einem (selbst-)lernenden System, das auf Ähnlichkeit basierend versucht, Artikel zu finden, die den Interessen des Users entsprechen\n",
    "* *User Interface*\n",
    "\n",
    "### Aufbau & Aufgaben\n",
    "\n",
    "#### 1. Daten: \n",
    "* Quelle(n) finden und stabiles Download von Daten gewährleisten\n",
    "* Daten bereinigen und in einem strukturierten Format alle wichtigen Informationen wie etwa Link, Quelle, Artikelkategorie, Beschreibung etc bereitstellen\n",
    "\n",
    "#### 2. Logik:\n",
    "* Texte in einer für die Bearbeitung geeigneter Form darstellen (vektorisieren)\n",
    "* Ein klassifizierendes System, das auf Interaktionen des Users reagiert und dementsprechend die zu erwartende Relevanz von Artikeln berechnet:  \n",
    "\n",
    "  * Artikel, die der Benutzer bereits gelesen hat, müssen für das Clustering verwendet werden. \n",
    "  * Auf den sich daraus ergebenden Klassenlabels wird ein klassifizierender Algorithmus trainiert.\n",
    "  * Letzter wiederum wird verwendet, um neue Nachrichten zu klassifizieren. \n",
    "  * Basierend auf den Klassenlabels und Klassengewichtungen sollten die neuen Nachrichten dem Benutzer in der \"personalisierten\" Kategorie angezeigt werden. \n",
    "  * Wenn eine genügende Anzahl an neuen Artikeln von dem Benutzer gelesen wurde -> Clustering erneut durchführen, Classifier und Clustergewichte aktualisieren.\n",
    "\n",
    "* Koordinierung der Systemkomponenten & Kommunikation mit GUI\n",
    "\n",
    "##### Classification & Clustering - Aufgaben:\n",
    "* Vektorrepräsentationen der Texte und Distanzen _(derzeit: Pre-trained Word2Vec; cosine distance)_ <br>\n",
    "$$cosine(u,v) = 1-\\frac{u\\cdot v}{\\left \\| u \\right \\|_{2}\\left \\| v \\right \\|_{2}}$$ <br>\n",
    "* Distance threshold für Agglomerative Clustering Modell anhand von Datenbeispielen finden (empirisch)  \n",
    "* Radius für Radius Neighbours Modell anhand von klassifizierten Datenbeispielen bestimmen\n",
    "\n",
    "\n",
    "#### 3. User Interface:\n",
    "\n",
    "* Funktionalität festlegen und Interface entwerfen\n",
    "* Interface bauen und mit den anderen Komponenten verbinden\n",
    "\n",
    "##### GUI - Funktionalität und Interaktionen:\n",
    "* Es werden Nachrichten-Items in verschiedenen **Kategorien (Sektionen)** angezeigt\n",
    "* Jedes **Nachricht-Item** besteht aus Titel, Summary, Link, Datum und Uhrzeit \n",
    "* Jedes Nachricht-Item besitzt außerdem einen **ID**, der zwar dem User nicht angezeigt wird aber für Kommunikation mit dem System benötigt wird\n",
    "* **Update-Button**, um Nachrichten erneut herunterzuladen\n",
    "* **Show similar-Button**, um ähnliche Nachrichten zu finden und zu zeigen\n",
    "* **Close-Button**, Daten werden gespeichert, Applikation beendet\n",
    "\n",
    "_Datenformat_\n",
    "\n",
    "* Nachrichten-Items werden als pandas-DataFrame Objekte bereitsgestellt\n",
    "* DataFrame enthält u.a. Spalten: `title`, `summary`, `link`, `datetime` und `ID`\n",
    "* `ID` ist derzeit ein hash-Code für jede Nachricht und sollte nicht angezeigt sondern nur für die innere Kommunikation mit dem System verwendet werden\n",
    "* Nachrichten aus den in rss-feeds pre-definierten Kategorien (wie Politik, Kultur, ...) werden gesamt als DataFrame mit entsprechender `category`-Spalte bereitgestellt\n",
    "* Nachrichten aus der personalisierten Kategorie (\"interesting for you/ based on recent viewed/...\") werden im separaten DataFrame bereitgestellt\n",
    "* DataFrames können auch **leer sein** bzw. einige Kategorien können nicht präsent sein (keine Einträge)\n",
    "\n",
    "_Interaktionen_\n",
    "\n",
    "0. Die **Applikation** wird **gestartet**:  \n",
    "    _Das System versucht, Daten und Modelle einzulesen, wenn OK -> das existierende System wird gestartet, wenn etwas fehlt -> ein neues wird initialisiert._\n",
    "    - Input System -> GUI: Zwei DataFrames - DataFrame mit pre-definierten Kategorien und DataFrame mit Nachrichten für die personalisierte Kategorie, bereitsgestellte Nachrichten-Items müssen nun angezeigt werden.\n",
    "    \n",
    "1. Der **Link** zur Nachricht wird **angeklickt**: \n",
    "    - Link muss geöffnet werden (z.B. in Browser, bitte überlegt euch, wir ihr das implementiert)\n",
    "    - Output GUI -> System: Nachricht-ID\n",
    "    - Input System -> GUI: Updated DataFrames mit Nachrichten-Items, die nun anstatt \"alten\" angezeigt werden sollen _(dabei wurde die angeklickte Nachricht als \"viewed\" wahrgenommen und deshalb entfernt; evtl hat sich das klassifizierende System verändert und andere Nachrichten als \"interessante\" gewählt)_\n",
    "  \n",
    "2. **Update-Button** wird **angeklickt**:\n",
    "    - Output GUI -> System: Irgendein Identifikator, z.B. string \"update\", der als entsprechender Befehl erkannt wird und woraufhin Nachrichten-Update durchgeführt wird\n",
    "    - Input System -> GUI: Zwei DataFrames - DataFrame mit pre-definierten Kategorien und DataFrame mit Nachrichten für die personalisierte Kategorie, bereitsgestellte Nachrichten-Items müssen nun angezeigt werden\n",
    " \n",
    "3. **Session** wird **beendet** (**Close-Button** angeklickt):\n",
    "    - Output GUI -> System: Entsprechender Identifikator, der als Befehl erkannt wird, dass die Daten und Modelle gespeichert werden müssen\n",
    "    - Input System -> GUI: boolean \n",
    "    - Applikation wird geschlossen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from scipy.spatial.distance import cosine, cityblock, jaccard, canberra, euclidean, minkowski, braycurtis\n",
    "stop_words = stopwords.words('english')\n",
    "from pyemd import emd\n",
    "from scipy.cluster.hierarchy import fclusterdata\n",
    "from sklearn.cluster import DBSCAN, AffinityPropagation, AgglomerativeClustering\n",
    "from sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier, kneighbors_graph\n",
    "import pickle\n",
    "import time\n",
    "import hashlib\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "#from itertools import combinations\n",
    "#from tqdm import tqdm_notebook\n",
    "#from scipy.stats import skew, kurtosis\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "#from sklearn.decomposition import PCA, TruncatedSVD, LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Worteinbettungen\n",
    "  \n",
    "- Word2Vec (pre-trained on Google News dataset, 300 dimensions)\n",
    "- Modell prinzipiell austauschbar\n",
    "- **Klasse `Embeddings`** kreiert oder liest ein (serialized) Modell ein, letzteres viel schneller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings:\n",
    "    \n",
    "    def __init__(self, create=False, ser_model_path='W2VModel',\n",
    "                 embeddings='GoogleNews-vectors-negative300.bin.gz',\n",
    "                 model_fun=gensim.models.KeyedVectors.load_word2vec_format, binary=True, norm=True):\n",
    "        self.ser_model = ser_model_path\n",
    "        self.embeddings = embeddings\n",
    "        self.model_fun = model_fun\n",
    "        self.binary = binary\n",
    "        self.norm = norm\n",
    "        \n",
    "        if create == False:\n",
    "            self.model = self.load_model()\n",
    "        else:\n",
    "            self.model = self.create_model()\n",
    "        \n",
    "            \n",
    "    def create_model(self):\n",
    "        model = self.model_fun(self.embeddings, binary=self.binary)\n",
    "        if self.norm:\n",
    "            model.init_sims(replace=True)\n",
    "        return model\n",
    "            \n",
    "    def load_model(self):\n",
    "        with open(self.ser_model, 'rb') as file:\n",
    "            model = pickle.load(file)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dokumente in Vektorraum abbilden\n",
    "  \n",
    "- Mit Word2Vec kann jedes Wort als ein 300-dimensionaler Vektor dargestellt werden\n",
    "- Dokumente (Nachrichten) beinhalten unterschiedliche Anzahl an Wörtern\n",
    "- Clusteringalgorithmen können direkt mit Distanzmatrizen arbeiten, für die Klassifizierungsaufgaben ist es allerdings sinnvoll, alle Dokumente als n-dimensionale Vektoren darzustellen\n",
    "\n",
    "Eine Nachricht, jedes Wort (Zeile) ist ein Vektor:\n",
    "\n",
    "$$ \\begin{pmatrix}\n",
    " x_{11}&  x_{12}&  ...& x_{1n}\\\\ \n",
    " x_{21}&  x_{22}&  ...& x_{2n}\\\\ \n",
    " &  & ... & \\\\ \n",
    " x_{m1}&  x_{m2}& ... &x_{mn} \n",
    "\\end{pmatrix} $$\n",
    "\n",
    "Vektor _**v**_ ist eine Summe von Wörterrepräsentationen:\n",
    "\n",
    "$$ v= \\left \\{ \\left. \\sum_{i=1}^{m}x_{i1}, \\sum_{i=1}^{m}x_{i2}, ...,  \\sum_{i=1}^{m}x_{in}\\right \\} \\right. $$\n",
    "\n",
    "Vektor _**v**_ wird normiert:\n",
    "\n",
    "$$\\frac{v}{\\sqrt{\\sum v_{i}^{2}}}$$\n",
    "\n",
    "#### Klasse `News_Vectorizer`  kann\n",
    "- Dokumente in Vektorraum abbilden\n",
    "- Distanzen zwischen dem vorgegebenen Dokument und einem Set aus Dokumenten berechnen\n",
    "- Distanzmatrizen berechnen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class News_Vectorizer:\n",
    "    \n",
    "    def __init__(self, model, news=None):\n",
    "        self.news = news #array of strings\n",
    "        self.model = model #Word2Vec model\n",
    "        if self.news is not None:\n",
    "            self.news_vectors = self.news2vec(self.news) #vector representations\n",
    "        else:\n",
    "            self.news_vectors = None\n",
    "        self.cos_dist = None #cosine distance matrix\n",
    "        self.wm_dist = None #wmd-matrix\n",
    "    \n",
    "    def wmd(self, q1, q2):\n",
    "        # Word Mover's distance\n",
    "        q1 = str(q1).lower().split()\n",
    "        q2 = str(q2).lower().split()\n",
    "        q1 = [w for w in q1 if w not in stop_words]\n",
    "        q2 = [w for w in q2 if w not in stop_words]\n",
    "        return self.model.wmdistance(q1, q2)\n",
    "    \n",
    "    def sent2vec(self, s):\n",
    "        #single document to vector\n",
    "        words = str(s).lower()\n",
    "        words = word_tokenize(words)\n",
    "        words = [w for w in words if not w in stop_words]\n",
    "        words = [w for w in words if w.isalpha()]\n",
    "        M = []\n",
    "        for w in words:\n",
    "            try:\n",
    "                M.append(self.model[w])\n",
    "            except:\n",
    "                continue\n",
    "        M = np.array(M)\n",
    "        v = M.sum(axis=0)\n",
    "        return v / np.sqrt((v ** 2).sum())\n",
    "    \n",
    "    def news2vec(self, news):\n",
    "        # update self.news, self.news_vectors\n",
    "        news_vectors = np.array([self.sent2vec(text) for text in news])\n",
    "        self.news = news\n",
    "        self.news_vectors = news_vectors\n",
    "        return news_vectors\n",
    "    \n",
    "    def dist_vec(self, news_item, news=None, metric='cosine'):\n",
    "        #computes distances between given item and news (or self.news)\n",
    "        news_item = self.sent2vec(news_item)\n",
    "        if news is not None:\n",
    "            news = self.news2vec(news)\n",
    "        else:\n",
    "            news = self.news_vectors\n",
    "        if news is None:\n",
    "            return 'no news to compute distances'\n",
    "        if metric == 'cosine':\n",
    "            dist_vec = np.array([cosine(news_item, i) for i in news])\n",
    "        elif metric == 'wmd':\n",
    "            dist_vec = np.array([self.wmd(news_item, i) for i in news])\n",
    "        return dist_vec\n",
    "    \n",
    "    # TODO: symmetric matrix - optimize\n",
    "    def cosine_matrix(self): \n",
    "        # cosine distance matrix\n",
    "        cdist = np.zeros((len(self.news_vectors), len(self.news_vectors)))\n",
    "        for n, i in enumerate(self.news_vectors):\n",
    "            for m, j in enumerate(self.news_vectors):\n",
    "                cdist[n, m] = cosine(i, j)\n",
    "        self.cos_dist = cdist\n",
    "        return cdist\n",
    "    \n",
    "    def wmd_matrix(self): #list (news)\n",
    "        # wmd distance matrix\n",
    "        wmdist = np.zeros((len(self.news), len(self.news)))\n",
    "        for n, i in enumerate(self.news):\n",
    "            for m, j in enumerate(self.news):\n",
    "                wmdist[n, m] = self.wmd(i, j)\n",
    "        self.wm_dist = wmdist\n",
    "        return wmdist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nachrichten aus rss feeds herunterladen und bearbeiten\n",
    "\n",
    "**Klasse `RSS_Feeds`:** \n",
    "  \n",
    "- Bekommt eine URL-Liste\n",
    "- Bearbeitet xml-feeds mit `feedparser`\n",
    "- Bildet ein Dataframe mit Nachrichten und dazugehörigen Informationen, wie das Datum, der Link etc.\n",
    "- Jeder Nachricht wird ein ID zugeordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSS_Feeds:\n",
    "    \n",
    "    def __init__(self, urls):\n",
    "        self.urls = urls\n",
    "        self.feeds = self.get_feeds()\n",
    "        self.df_news = self.create_df()\n",
    "        self.df_unique_news = self.create_unique()\n",
    "        \n",
    "    def get_feeds(self):\n",
    "        return [feedparser.parse(feed) for feed in self.urls]\n",
    "    \n",
    "    def get_category(self, feed):\n",
    "        # sources may have different category names - agg categories?\n",
    "        return feed.feed.get('title', '')\n",
    "\n",
    "    def get_title_summary(self, feed, sep='. '): #get and join title and summary for each entry in feed\n",
    "        titles = [entry['title'] for entry in feed['entries']]\n",
    "        summaries = [entry['summary'] for entry in feed['entries']]\n",
    "        title_summary = [entry['title'] + sep + entry['summary'] for entry in feed['entries']]\n",
    "        return titles, summaries, title_summary\n",
    "    \n",
    "    def get_date(self, feed): #(year, month, day) for each entry in feed\n",
    "        return([entry['published_parsed'][:3] for entry in feed['entries']])\n",
    "    \n",
    "    def get_time(self, feed): #(hour, min, sec) for each entry in feed\n",
    "        return([entry['published_parsed'][3:6] for entry in feed['entries']])\n",
    "    \n",
    "    def get_datetime_nparsed(self, feed): #not parsed date and time for each entry in feed\n",
    "        return([entry['published'] for entry in feed['entries']])\n",
    "    \n",
    "    def get_link(self, feed): # link for each entry in feed\n",
    "        return([entry['link'] for entry in feed['entries']])\n",
    "    \n",
    "    def str2hash(self, s):\n",
    "        return hashlib.md5(s.encode()).hexdigest()\n",
    "    \n",
    "    def create_df(self): \n",
    "        news, title, summary, category, pdate, ptime, fdatetime, links  = [], [], [], [], [], [], [], []\n",
    "        for feed in self.feeds:\n",
    "            cat = self.get_category(feed)\n",
    "            titles, summaries, texts = self.get_title_summary(feed)\n",
    "            d_ymd, t_hms = self.get_date(feed), self.get_time(feed)\n",
    "            fdt = self.get_datetime_nparsed(feed)\n",
    "            news_links = self.get_link(feed)\n",
    "            \n",
    "            cat = np.resize([cat], len(texts))\n",
    "            news.extend(texts)\n",
    "            title.extend(titles)\n",
    "            summary.extend(summaries)\n",
    "            pdate.extend(d_ymd)\n",
    "            ptime.extend(t_hms)\n",
    "            fdatetime.extend(fdt)\n",
    "            links.extend(news_links)\n",
    "            category.extend(cat)\n",
    "        df_news = pd.DataFrame({'news':news, \n",
    "                                'category':category,\n",
    "                                'title':title, \n",
    "                                'summary':summary,\n",
    "                                'link':links,\n",
    "                                'date':pdate, \n",
    "                                'time':ptime, \n",
    "                                'datetime':fdatetime})\n",
    "        df_news['ID'] = df_news.news.apply(self.str2hash)\n",
    "        self.df_news = df_news\n",
    "        return df_news\n",
    "    \n",
    "    def create_unique(self):\n",
    "        df_unique_news = self.df_news.groupby('news').agg({'category':list, \n",
    "                                                           'title': np.unique, \n",
    "                                                           'summary': np.unique, \n",
    "                                                           'link': np.unique, \n",
    "                                                           'date': np.unique, \n",
    "                                                           'time': np.unique, \n",
    "                                                           'datetime': np.unique, \n",
    "                                                           'ID': np.unique})\n",
    "        df_unique_news.reset_index(inplace=True)\n",
    "        self.df_unique_news = df_unique_news\n",
    "        return df_unique_news\n",
    "    \n",
    "    def get_unique_news(self):\n",
    "        return self.df_unique_news.news.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gelesene Nachrichten gruppieren und neue Nachrichten entsprechend klassifizieren  \n",
    "\n",
    "- Die bereits vom Benutzer gelesenen Nachrichten werden mit Clustering gruppiert, um einzelne Themenbereiche zu finden, die dem Benutzer möglicherweise interessant sind\n",
    "- Auf diesen Daten wird ein klassifizierendes Modell trainiert\n",
    "\n",
    "- Jeder Cluster bekommt ein Gewicht _(derzeit basierend auf der Anzahl Beobachtungen im jeweiligen Cluster)_\n",
    "- Neue, noch nicht gelesene Nachrichten werden mit dem trainierten Klassifizierungsmodell einer der durch Clustering gefundenen Gruppen oder einem \"outlier\"-Cluster zugeordnet\n",
    "- Dieser Klassifizierung und den Clustergewichten entsprechend werden Nachrichten für die personalisierte Kategorie _\"Interesting\"_ ausgewählt\n",
    "  \n",
    "**Klasse `Aggregator`:**  \n",
    " \n",
    "- Bekommt Clustering- und Klassifizierungsmodell und, wenn bereits vorhanden, Daten mit Clusterlabels und Clustergewichte\n",
    "- Methode `update_aggregator` bekommt neue Daten, führt Clustering erneut durch (auf den neuen und bereits vorhandenen Daten gesamt), trainiert Classifier, aktualisiert Clustergewichte\n",
    "- Methode `classify` verwendet das Klassifizierungsmodell, um neue Nachrichten entsprechenden Clustern oder einem \"outlier\"-Cluster zuzuordnen\n",
    "\n",
    "**Modelle**:\n",
    "  \n",
    "- Sind prinzipiell gut austauschbar und können bestimmt besser angepasst werden\n",
    "- Derzeit werden verwendet: `AgglomerativeClustering` und `RadiusNeighborsClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Aggregator:\n",
    "    \n",
    "    def __init__(self, clusterizer, classifier, labeled_data=None, labels=None, clust_weights=None):\n",
    "        self.clusterizer = clusterizer\n",
    "        self.classifier = classifier\n",
    "        self.labeled_data = labeled_data #already clustered viewed, vector representations as ndarray\n",
    "        self.labels = labels #clust nums of labeled_data, ndarray\n",
    "        self.clust_weights = clust_weights # DataFrame, colnames=['clust', 'weight']\n",
    "        \n",
    "    def clusterize(self, data):\n",
    "        labels = self.clusterizer.fit_predict(data)\n",
    "        return data, labels\n",
    "    \n",
    "    def classify(self, new_data): #if one sample: reshape sent2vec output to (1, 300)\n",
    "        try:\n",
    "            predicted = self.classifier.predict(new_data)\n",
    "        except NotFittedError as e:\n",
    "            return(repr(e))\n",
    "        return predicted\n",
    "    \n",
    "    def fit_classifier(self):\n",
    "        X, y = self.labeled_data, self.labels\n",
    "        self.classifier.fit(X, y)\n",
    "        return self.classifier\n",
    "    \n",
    "    def prep_data(self, new_data=None):\n",
    "        if self.labeled_data is None and new_data is None:\n",
    "            return None\n",
    "        else:\n",
    "            try:\n",
    "                ldata = pd.DataFrame(self.labeled_data)\n",
    "            except:\n",
    "                ldata = None\n",
    "            try:\n",
    "                ndata = pd.DataFrame(new_data)\n",
    "            except:\n",
    "                ndata=None\n",
    "            try:\n",
    "                data = pd.concat([ldata, ndata]).values\n",
    "                return data\n",
    "            except:\n",
    "                return None\n",
    "    \n",
    "    def update_weights(self): #sum weights = 1 required in News_Finder\n",
    "        unique, counts = np.unique(self.labels[self.labels != -1], return_counts=True)\n",
    "        weights = counts/counts.sum()\n",
    "        weights = np.asarray((unique, weights)).T # [label, weight]\n",
    "        self.clust_weights = pd.DataFrame({'clust': weights[:,0].astype(int), 'weight': weights[:,1]})\n",
    "        return self.clust_weights\n",
    "    \n",
    "    def update_aggregator(self, new_data):\n",
    "        data = self.prep_data(new_data=new_data)\n",
    "        if data is None:\n",
    "            return 'no data'\n",
    "        else:\n",
    "            self.labeled_data, self.labels = self.clusterize(data)    \n",
    "            self.fit_classifier()\n",
    "            self.update_weights()\n",
    "            return 'updated'\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nachrichten für das Anzeigen finden  \n",
    "  \n",
    "**Klasse `News_Finder`**:\n",
    "\n",
    "- `get_from_categories` gibt ein Dataframe mit `n` Nachrichten aus jeder Kategorie aus\n",
    "- `get_interesting` lässt noch nicht gelesene Nachrichten klassifizieren und gibt ein Dataframe mit Nachrichten aus, die den Interessenbereichen des Benutzers zugeordnet werden konnten\n",
    "- `get_similar` lässt Distanzen zwischen der vorgegebenen Nachricht und anderen Nachrichten berechnen und findet `n` nächsten (ähnlichsten) Dokumente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class News_Finder():\n",
    "    \n",
    "    def __init__(self, df_news, news_vectorizer): #df_news: DF with non-viewed news items; News_Vectorizer instance\n",
    "        self.df_news = df_news\n",
    "        self.df_unique_news = self.create_unique()\n",
    "        self.news_vectorizer = news_vectorizer\n",
    "        \n",
    "    def update_news(self, df_news):\n",
    "        self.df_news = df_news\n",
    "        self.df_unique_news = self.create_unique()\n",
    "        return 'updated'\n",
    "    \n",
    "    def create_unique(self):\n",
    "        df_unique_news = self.df_news.groupby('ID').agg({'news': np.unique, \n",
    "                                                         'category':list, \n",
    "                                                         'title': np.unique, \n",
    "                                                         'summary': np.unique, \n",
    "                                                         'link': np.unique, \n",
    "                                                         'date': np.unique, \n",
    "                                                         'time': np.unique, \n",
    "                                                         'datetime': np.unique})\n",
    "        df_unique_news.reset_index(inplace=True)\n",
    "        return df_unique_news\n",
    "    \n",
    "    def get_from_categories(self, n=5):\n",
    "        # returns n top news from each category\n",
    "        return self.df_news.groupby('category').head(n) \n",
    "    \n",
    "    def get_similar(self, news_item_ID, metric='cosine', n=5):\n",
    "        # returns the n most similar news to news_item\n",
    "        all_news = self.df_unique_news.query('ID != @news_item_ID').copy()\n",
    "        news_item = self.df_unique_news.query('ID == @news_item_ID').copy()\n",
    "        dist_vec = self.news_vectorizer.dist_vec(news_item.news, all_news.news.values, metric=metric)\n",
    "        all_news['dist'] = dist_vec\n",
    "        return all_news.nsmallest(n, 'dist')\n",
    "    \n",
    "    def get_interesting(self, aggregator, n=20): #fitted Aggregator instance for classification & weights\n",
    "        # TODO: return n news\n",
    "        # TODO: if there are not enough news in clusters (for weights), return another news?\n",
    "        all_news = self.df_unique_news.copy()\n",
    "        news_vec = self.news_vectorizer.news2vec(all_news.news.values)\n",
    "        #print(news_vec.shape)\n",
    "        weights = aggregator.clust_weights\n",
    "        labels = aggregator.classify(news_vec)\n",
    "        all_news['label'] = labels\n",
    "        all_news = all_news.query('label != -1')\n",
    "        n_from_cluster = np.ceil((aggregator.clust_weights.weight*n)).astype(int)\n",
    "        dflist = []\n",
    "        for cluster, n in zip(weights.clust, n_from_cluster):\n",
    "            dflist.append(all_news.query('label == @cluster').head(n))\n",
    "        interesting = pd.concat(dflist)\n",
    "        return interesting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Management von Daten & Modellen  \n",
    "\n",
    "**Klasse `Data_Manager`**:\n",
    "  \n",
    "- Bekommt ein Dictionary Objekt `path_dict` und liest Daten ein\n",
    "- Speichert Daten und Modelle in `data_dict` und gibt mit der Methode `get_data_item` das erforderliche Objekt aus\n",
    "- Aktualisiert Objekte mit `update_data_item`-Methode\n",
    "- Löscht alte Einträge (außer vorgegebenen `n_recent`) aus Dataframes mit `delete_old`\n",
    "- Speichert Daten mit `save_model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_Manager:\n",
    "    \n",
    "    def __init__(self, path_dict=None): #path_dict {'csv':{obj:path}, 'serialized':{obj:path}}\n",
    "        self.path_dict = path_dict\n",
    "        if self.path_dict is not None:\n",
    "            self.data_dict = self.load_data()\n",
    "        else:\n",
    "            self.data_dict = {}\n",
    "        \n",
    "    def load_data(self):\n",
    "        data_dict = {}\n",
    "        try:\n",
    "            for obj_name, path in self.path_dict['csv'].items():\n",
    "                data_dict[obj_name] = pd.read_csv(path, index_col=0)\n",
    "        except:\n",
    "            print('something is not ok with \"csv\" key or it does not exist')\n",
    "        try:\n",
    "            for obj_name, path in self.path_dict['serialized'].items():\n",
    "                with open(path, 'rb') as file:\n",
    "                    data_dict[obj_name] = pickle.load(file)\n",
    "        except:\n",
    "            print('something is not ok with \"serialized\" key or it does not exist')\n",
    "        return data_dict\n",
    "    \n",
    "    def delete_old(self, obj_name, n_recent=100):\n",
    "        #del old data, except n_recent\n",
    "        data = self.get_data_item(obj_name)\n",
    "        if type(data) == pd.core.frame.DataFrame and data.shape[0] > n_recent:\n",
    "            data = data.tail(n_recent)\n",
    "            self.update_data_item(obj_name, data, concat=False)\n",
    "            return('old entries removed')\n",
    "        return('not enough entries to delete or is not DF')\n",
    "    \n",
    "    #def prep_data(self):\n",
    "    #    #maybe some data manipulations\n",
    "    #    pass\n",
    "    \n",
    "    def get_data_item(self, obj_name):\n",
    "        return self.data_dict.get(obj_name, 'Does not exist')\n",
    "    \n",
    "    def update_data_item(self, obj_name, new_data, concat=False): #concat [True, False] - if concat data\n",
    "        if concat == False:\n",
    "            self.data_dict[obj_name] = new_data\n",
    "            return('upd: set data_dict[obj] = new_data')\n",
    "        elif concat == True:\n",
    "            if obj_name in self.data_dict.keys():\n",
    "                data = self.get_data_item(obj_name)\n",
    "                if type(data) == pd.core.frame.DataFrame:\n",
    "                    try:\n",
    "                        data = pd.concat([data, new_data], sort=False)\n",
    "                        self.data_dict[obj_name] = data\n",
    "                        return 'updated'\n",
    "                    except:\n",
    "                        return 'could not update'\n",
    "                elif type(data) == np.ndarray:\n",
    "                    try:\n",
    "                        data = np.vstack([data, new_data])\n",
    "                        self.data_dict[obj_name] = data\n",
    "                        return 'updated'\n",
    "                    except:\n",
    "                        return 'could not update'\n",
    "                else:\n",
    "                    self.data_dict[obj_name] = new_data\n",
    "                    return 'upd: obj = new_data (not an array or DF)'\n",
    "            else:\n",
    "                self.data_dict[obj_name] = new_data\n",
    "                return 'upd: obj = new_data (obj did not exist yet)'\n",
    "    \n",
    "    def save_model(self, data_items='all'): #data_items: 'all' or list of keys for data_dict\n",
    "        if data_items == 'all':\n",
    "            data_items = self.data_dict.keys()\n",
    "        for obj_name in data_items:\n",
    "            data = self.data_dict[obj_name]\n",
    "            if type(data) == pd.core.frame.DataFrame:\n",
    "                data.to_csv(obj_name + '.csv')\n",
    "            elif type(data) == np.ndarray:\n",
    "                # are there any ndarrays?..\n",
    "                # TODO: write csv...\n",
    "                pass\n",
    "            else:\n",
    "                with open(obj_name, 'wb') as file:\n",
    "                    pickle.dump(data, file)\n",
    "        return 'saved'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Koordinierung von einzelnen Komponenten und Schnittstelle zu GUI\n",
    "\n",
    "Erforderliche Funktionalität wird von der **Klasse `Communicator`** unter Verwendung von den oben definierten Komponenten bereitgestellt. Die Klasse implementiert die Applikationslogik und hat die `handle_input`-Methode, die eine einfache Kommunikation mit dem Interface ermöglicht.\n",
    "\n",
    "- **Generell kann sich das System in drei Zuständen befinden (`STATE`): **\n",
    "  1. `ALL`: Alle Daten & Modelle vorhanden (was bedeutet, dass das Klassifizierungsmodell bereits trainiert (fitted) worden ist und neue Nachrichten dementsprechend klassifiziert werden können)\n",
    "  2. `NOT FITTED`: Das System wurde bereits initialisiert, allerdings wurde der Classifier noch nicht trainiert und es sind nicht alle Daten vorhanden\n",
    "  3. `NEW`: Keine Daten vorhanden, ein komplett neues System ist initialisiert worden  \n",
    "  \n",
    "  \n",
    "- Die **`start`-Methode** versucht, Daten und Modelle einzulesen. Je nachdem, in welchem Zustand sich das System befindet, wird die Vorgehensweise gewählt. Nur ein System mit dem bereits trainierten Classifier (`STATE == \"ALL\"`) kann relevante Nachrichten finden. Ansonsten wird für diese Kategorie ein zufälliges Sample ausgewählt.\n",
    "`start` gibt zwei Dataframes aus:\n",
    "\n",
    "``` \n",
    "CI = Communicator() \n",
    "news_in_categories, news_interesting = CI.start()\n",
    "\n",
    "```\n",
    "\n",
    "- **Methode `handle_input` ist eine Schnittstelle zu GUI und akzeptiert folgende Eingaben:**\n",
    "  1. `'upd'`: Nachrichten aktualisieren (erneut herunterladen).   \n",
    "     Output: Zwei Dataframes (news in categories, interesting news)\n",
    "       \n",
    "  2. `'exit'`: Daten und Modelle speichern.  \n",
    "     Output: True, wenn Daten erfolgreich gespeichert, sonst False.  \n",
    "       \n",
    "  3. `'viewed' + ' ' + ID`: Bearbeitet gelesene Nachrichten: Sie werden in dem \"viewed\"-Dataframe gespeichert und aus den \"allen\" Nachrichten rausgenommen. Wenn in \"viewed\"-Dataframe genug Nachrichten vorhanden sind (`THRESHOLD` Parameter), wird die `Aggregator`-Instanz aktualisiert: Clustering erneut durchgeführt und Classifier trainiert. In diesem Fall wird der aktueller Stand des Systems (alle Daten und Modelle) automatisch gespeichert.  \n",
    "     Output: Zwei Dataframes (news in categories, interesting news)  \n",
    "     \n",
    "  4. `'similar' + ' ' + ID`: Findet die ähnlichsten Nachrichten anhand von Distanzen.   \n",
    "     Output: Dataframe           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Communicator:\n",
    "    '''Application start -> initialize Communicator instance and call `start()`\n",
    "    to start an existing system or create a new system and get DataFrames, e.g.:\n",
    "    \n",
    "    `CI = Communicator()\n",
    "    news_in_categories, interesting_news = CI.start()\n",
    "    display(news_in_categories)`\n",
    "    \n",
    "    Then use `handle_input()` to process user input and get system output, e.g.:\n",
    "    \n",
    "    `news_in_categories, interesting_news = CI.handle_input(u_input='upd')` '''   \n",
    "    \n",
    "    def __init__(self, feeds='default', model='default', path_dict='default', data_items_names='default', \n",
    "                 n_from_cats=10, n_interesting=20, n_similar=5, THRESHOLD=20):\n",
    "        \n",
    "        if feeds == 'default':\n",
    "            self.feeds = ['http://feeds.bbci.co.uk/news/rss.xml', \n",
    "                          'http://feeds.bbci.co.uk/news/world/rss.xml', \n",
    "                          'http://feeds.bbci.co.uk/news/uk/rss.xml', \n",
    "                          'http://feeds.bbci.co.uk/news/business/rss.xml', \n",
    "                          'http://feeds.bbci.co.uk/news/politics/rss.xml', \n",
    "                          'http://feeds.bbci.co.uk/news/health/rss.xml', \n",
    "                          'http://feeds.bbci.co.uk/news/education/rss.xml', \n",
    "                          'http://feeds.bbci.co.uk/news/science_and_environment/rss.xml', \n",
    "                          'http://feeds.bbci.co.uk/news/technology/rss.xml', \n",
    "                          'http://feeds.bbci.co.uk/news/entertainment_and_arts/rss.xml']\n",
    "        else:\n",
    "            self.feeds = feeds\n",
    "            \n",
    "        if model == 'default':\n",
    "            self.model = Embeddings().model\n",
    "        else:\n",
    "            self.model = model\n",
    "        \n",
    "        if path_dict == 'default':\n",
    "            self.path_dict = {'csv':{'df_viewed':'df_viewed.csv', \n",
    "                                     'df_labeled':'df_labeled.csv', \n",
    "                                     'clust_weights':'clust_weights.csv'}, \n",
    "                              'serialized':{'classifier':'classifier', \n",
    "                                            'clusterizer':'clusterizer'}}\n",
    "        else:\n",
    "            self.path_dict = path_dict\n",
    "            \n",
    "        if data_items_names == 'default':\n",
    "            self.data_items_names = ['df_viewed', 'df_labeled', 'clust_weights', 'classifier', 'clusterizer']\n",
    "        else:\n",
    "            self.data_items_names = data_items_names\n",
    "        \n",
    "        self.STATE = None\n",
    "        self.n_from_cats = n_from_cats\n",
    "        self.n_interesting = n_interesting\n",
    "        self.n_similar = n_similar\n",
    "        self.THRESHOLD = THRESHOLD\n",
    "        \n",
    "        self.DM = None\n",
    "        self.NFind = None\n",
    "        self.AGG = None\n",
    "        \n",
    "        self.RSS = RSS_Feeds(self.feeds)\n",
    "        self.NVec = News_Vectorizer(model=self.model)\n",
    "        \n",
    "        self.init_classification_model = RadiusNeighborsClassifier(radius=0.49, weights='distance', \n",
    "                                                                   metric='cosine', outlier_label='-1')\n",
    "        self.init_clustering_model = AgglomerativeClustering(n_clusters=None, affinity='cosine', \n",
    "                                                             linkage='complete', distance_threshold=0.65)\n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "    def create_new_system(self):\n",
    "        # create a new system without any already existing data\n",
    "        self.DM = Data_Manager()\n",
    "        self.AGG = Aggregator(self.init_clustering_model, self.init_classification_model)\n",
    "        \n",
    "        self.DM.update_data_item('df_news', self.RSS.df_news, concat=False)\n",
    "        self.DM.update_data_item('df_viewed', pd.DataFrame(columns=self.RSS.df_news.columns), concat=False) # DF for viewed news\n",
    "        self.DM.update_data_item('classifier', self.AGG.classifier, concat=False)\n",
    "        self.DM.update_data_item('clusterizer', self.AGG.clusterizer, concat=False)\n",
    "        return True\n",
    "    \n",
    "    def load_data_models(self):\n",
    "        # try to load data and check if all data items in data_items_names in data_dict\n",
    "        # set STATE ('ALL' - already fitted model, all data; 'NOT FITTED' - not fitted yet, some data; 'NEW' - no data)\n",
    "\n",
    "        self.DM = Data_Manager(path_dict=self.path_dict)\n",
    "        if len([i for i in self.data_items_names if i not in self.DM.data_dict.keys()]) == 0:\n",
    "            return ('ALL')\n",
    "        elif ('df_viewed' in self.DM.data_dict.keys()) and ('clust_weights' not in self.DM.data_dict.keys()):\n",
    "            return 'NOT FITTED'\n",
    "        else:\n",
    "            return 'NEW'\n",
    "        \n",
    "    def select_news(self):\n",
    "        categories = self.NFind.get_from_categories(n=self.n_from_cats)\n",
    "        if self.STATE == 'ALL':\n",
    "            interesting = self.NFind.get_interesting(self.AGG, n=self.n_interesting)\n",
    "        else:\n",
    "            interesting = self.NFind.df_unique_news.sample(self.n_interesting)\n",
    "        return categories, interesting\n",
    "    \n",
    "    def check_viewed(self):\n",
    "        # check and remove already viewed news from news DataFrame\n",
    "        if self.STATE == 'ALL':\n",
    "            viewed_id = np.hstack([self.DM.get_data_item('df_labeled').ID.values, self.DM.get_data_item('df_viewed').ID.values])\n",
    "        elif self.STATE == 'NOT FITTED':\n",
    "            viewed_id = self.DM.get_data_item('df_viewed').ID.values\n",
    "        self.DM.update_data_item('df_news', self.RSS.df_news.query('ID not in @viewed_id'), concat=False)\n",
    "        return True\n",
    "    \n",
    "    def start(self):\n",
    "        #try to load data\n",
    "        #if 'ALL' -> start existing\n",
    "        #if 'NEW' -> call `create_new_system` and change STATE\n",
    "        #if 'NOT FITTED' -> init aggregator with existing models\n",
    "        # return DataFrames\n",
    "        \n",
    "        self.STATE = self.load_data_models()\n",
    "        \n",
    "        if self.STATE == 'NOT FITTED':\n",
    "            self.AGG = Aggregator(clusterizer=self.DM.get_data_item('clusterizer'), \n",
    "                                  classifier=self.DM.get_data_item('classifier'))\n",
    "            # filter already viewed news\n",
    "            self.check_viewed()          \n",
    "            \n",
    "        elif self.STATE == 'ALL':\n",
    "            self.DM.delete_old('df_labeled')    \n",
    "            self.AGG = Aggregator(clusterizer=self.DM.get_data_item('clusterizer'), \n",
    "                                  classifier=self.DM.get_data_item('classifier'), \n",
    "                                  labeled_data=self.NVec.news2vec(self.DM.get_data_item('df_labeled').news.values), \n",
    "                                  labels=self.DM.get_data_item('df_labeled').label.values, \n",
    "                                  clust_weights=self.DM.get_data_item('clust_weights'))\n",
    "            # filter already viewed news\n",
    "            self.check_viewed()\n",
    "        \n",
    "        elif self.STATE == 'NEW':\n",
    "            self.create_new_system()\n",
    "            self.STATE = 'NOT FITTED'\n",
    "        \n",
    "        #initialize News_Finder\n",
    "        self.NFind = News_Finder(self.DM.get_data_item('df_news'), News_Vectorizer(model=self.model))\n",
    "        \n",
    "        # get news DataFrames to show\n",
    "        news_from_categories, news_interesting = self.select_news()\n",
    "            \n",
    "        return news_from_categories, news_interesting\n",
    "    \n",
    "\n",
    "    def handle_input(self, u_input): # the main method that GUI has to call\n",
    "        ''' Call this method with `u_input` argument to communicate with the system.\n",
    "            It takes a string `u_input` and returns appropriate output.\n",
    "            \n",
    "            Interactions as `u_input` -> `method output`:\n",
    "            \n",
    "            * 'upd' -> two pandas DataFrame objects (news in categories, interesting news)\n",
    "            * 'exit' -> boolean: True, if data and models have been successfully saved, False otherwise\n",
    "            * 'viewed' + ' ' + ID (e.g. 'viewed 005503512f38f130303cb133d656203b') -> two pandas DataFrame objects (news in categories, interesting news)\n",
    "            * 'similar' + ' ' + ID (e.g. 'similar dc313bbf1bfca18d28e95862e972822f') -> pandas DataFrame with nearest news            \n",
    "        '''\n",
    "        # parse GUI input\n",
    "        # call appropriate methods\n",
    "        # return system output to GUI\n",
    "        instruction = u_input.split()[0]\n",
    "        if instruction == 'upd':\n",
    "            #update news\n",
    "            categories, interesting = self.update_news()\n",
    "            return categories, interesting\n",
    "        \n",
    "        elif instruction == 'exit':\n",
    "            is_saved = self.save()\n",
    "            return is_saved\n",
    "        \n",
    "        elif instruction == 'viewed':\n",
    "            #get news id and process viewed...\n",
    "            n_id = u_input.split()[1:] # list, even if it consists of only one ID (generally)\n",
    "            categories, interesting = self.handle_viewed(n_id)\n",
    "            return categories, interesting\n",
    "\n",
    "        elif instruction == 'similar':\n",
    "            #get id and return similar\n",
    "            n_id = u_input.split()[1] # string\n",
    "            similar_news = self.find_similar(n_id)\n",
    "            return similar_news\n",
    "\n",
    "        else:\n",
    "            return('unknown input')\n",
    "        \n",
    "    \n",
    "    def update_news(self):\n",
    "        # download news from rss feeds\n",
    "        self.RSS = RSS_Feeds(self.feeds)\n",
    "        \n",
    "        # filter already viewed news\n",
    "        self.check_viewed()\n",
    "        \n",
    "        # update news in News_Finder\n",
    "        self.NFind.update_news(self.DM.get_data_item('df_news'))\n",
    "        # return DataFrames\n",
    "        news_from_categories, news_interesting = self.select_news()\n",
    "            \n",
    "        return news_from_categories, news_interesting\n",
    "    \n",
    "    def save(self):\n",
    "        try:\n",
    "            self.DM.save_model()\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def handle_viewed(self, n_id): #n_id : list\n",
    "        # get news ID, check if already in viewed, remove from all news,.....\n",
    "        viewed = self.NFind.df_unique_news.query('ID in @n_id')\n",
    "        self.NFind.update_news(self.NFind.df_news.query('ID not in @n_id')) #filter already viewed and update data\n",
    "        self.DM.update_data_item('df_viewed', viewed, concat=True)\n",
    "        self.DM.update_data_item('df_news', self.NFind.df_news, concat=False)\n",
    "\n",
    "        # if n(viewed) >= THRESHOLD -> update model -> save model\n",
    "        if self.DM.get_data_item('df_viewed').shape[0] >= self.THRESHOLD:\n",
    "            #update aggregator\n",
    "            data = self.DM.get_data_item('df_viewed')\n",
    "            colnames = data.columns\n",
    "            self.AGG.update_aggregator(self.NVec.news2vec(data.news))\n",
    "            \n",
    "            #update df_labeled\n",
    "            if self.STATE == 'ALL':\n",
    "                data = pd.concat([self.DM.get_data_item('df_labeled').drop('label', axis=1), data])\n",
    "            data['label'] = self.AGG.labels\n",
    "            self.DM.update_data_item('df_labeled', data, concat=False)\n",
    "            \n",
    "            #update df_viewed (empty)\n",
    "            self.DM.update_data_item('df_viewed', pd.DataFrame(columns=colnames), concat=False)\n",
    "            \n",
    "            #update data_items: classifier, clusterizer etc\n",
    "            self.DM.update_data_item('classifier', self.AGG.classifier, concat=False)\n",
    "            self.DM.update_data_item('clusterizer', self.AGG.clusterizer, concat=False)\n",
    "            self.DM.update_data_item('clust_weights', self.AGG.clust_weights, concat=False)\n",
    "            \n",
    "            # update STATE\n",
    "            self.STATE = 'ALL'\n",
    "            \n",
    "            #save model\n",
    "            self.save()\n",
    "            \n",
    "        # return DataFrames\n",
    "        news_from_categories, news_interesting = self.select_news()\n",
    "        return news_from_categories, news_interesting\n",
    "    \n",
    "    \n",
    "    def find_similar(self, n_id): #n_id : str\n",
    "        # return the n nearest news to the given news item\n",
    "        return self.NFind.get_similar(news_item_ID=n_id, n=self.n_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from IPython.core.display import display, HTML, clear_output\n",
    "import webbrowser\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_item(value, placeholder='', description=''):\n",
    "    w = widgets.HTML(\n",
    "            value=value,\n",
    "            placeholder=placeholder,\n",
    "            description=description\n",
    "        )\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_similar(df):\n",
    "    clear_output()\n",
    "    display(widgets.HBox([UpdButton(), CloseButton()]))\n",
    "    litems = [create_item('<b>'+str(i[1].title)+'</b>  <i>(' + str(i[1].datetime) + ')</i><br>'+str(i[1].summary) + '<br>') for i in df.iterrows()]\n",
    "    ritems = [widgets.HBox([LinkButton(ID=i[1].ID, link=i[1].link), ShowSimilarButton(ID=i[1].ID)]) for i in df.iterrows()]\n",
    "    items = []\n",
    "    for i, j in zip(litems, ritems):\n",
    "        items.append(i)\n",
    "        items.append(j)\n",
    "    display(widgets.GridBox(items, layout=widgets.Layout(grid='none / repeat(2)')))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_update(df_categories, df_interest):\n",
    "    clear_output()\n",
    "    print(time.ctime())\n",
    "    display(widgets.HBox([UpdButton(), CloseButton()]))   \n",
    "    cat_title = create_item('<br><h2>Interesting</h2>')\n",
    "    litems = [create_item('<b>'+str(i[1].title)+'</b>  <i>(' + str(i[1].datetime) + ')</i><br>'+str(i[1].summary) + '<br>') for i in df_interest.iterrows()]\n",
    "    ritems = [widgets.HBox([LinkButton(ID=i[1].ID, link=i[1].link), ShowSimilarButton(ID=i[1].ID)]) for i in df_interest.iterrows()]\n",
    "    items = []\n",
    "    for i, j in zip(litems, ritems):\n",
    "        items.append(i)\n",
    "        items.append(j)\n",
    "    display(cat_title)\n",
    "    display(widgets.GridBox(items, layout=widgets.Layout(grid_template_columns=\"repeat(2\")))\n",
    "    #display(widgets.GridBox(items, layout=widgets.Layout(grid_template_columns=\"repeat(2, 700px)\")))\n",
    "    #display(widgets.GridBox(items, layout=widgets.Layout(grid='none / repeat(1)')))\n",
    "    \n",
    "    for cat in df_categories.category.unique():\n",
    "        df = df_categories.query('category == @cat')#.head(10)\n",
    "        litems = [create_item('<b>'+str(i[1].title)+'</b>  <i>(' + str(i[1].datetime) + ')</i><br>'+str(i[1].summary) + '<br>') for i in df.iterrows()]\n",
    "        ritems = [widgets.HBox([LinkButton(ID=i[1].ID, link=i[1].link), ShowSimilarButton(ID=i[1].ID)]) for i in df.iterrows()]\n",
    "        cat_title = create_item('<br><h2>' + str(cat) + '</h2>')\n",
    "        items = []\n",
    "        for i, j in zip(litems, ritems):\n",
    "            items.append(i)\n",
    "            items.append(j)\n",
    "        display(cat_title)\n",
    "        #display(widgets.GridBox(items, layout=widgets.Layout(grid_template_columns=\"repeat(2, 700px)\")))\n",
    "        display(widgets.GridBox(items, layout=widgets.Layout(grid='none / repeat(2)')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upd_button_clicked(b):\n",
    "    cat, interest = CI.handle_input('upd')\n",
    "    view_update(cat, interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkButton(widgets.Button):\n",
    "    \n",
    "    global CI\n",
    "    global view_update\n",
    "    \n",
    "    def __init__(self, ID, link='https://www.google.com/', description='Read', disabled=False, style='', tooltip='Link to article', icon='link', *args, **kwargs):\n",
    "        \"\"\"Initialize the LinkButton class.\"\"\"\n",
    "        super(LinkButton, self).__init__(*args, **kwargs)\n",
    "        # Create the button.\n",
    "        self.link = link\n",
    "        self.ID = ID\n",
    "        self.description = description\n",
    "        self.disabled = disabled\n",
    "        #self.style = style\n",
    "        self.tooltip = tooltip\n",
    "        self.icon = \"link\"\n",
    "        self.style.button_color = \"lightblue\"\n",
    "        # Set on click behavior.\n",
    "        self.on_click(self.process)\n",
    "\n",
    "    def process(self, b):\n",
    "        webbrowser.open(self.link)\n",
    "        cat, interest = CI.handle_input('viewed '+ self.ID)\n",
    "        view_update(cat, interest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpdButton(widgets.Button):\n",
    "    \n",
    "    global CI\n",
    "    global view_update\n",
    "    \n",
    "    def __init__(self, description='Update', disabled=False, style='', tooltip='Download news', icon='refresh', *args, **kwargs):\n",
    "        \"\"\"Initialize the LinkButton class.\"\"\"\n",
    "        super(UpdButton, self).__init__(*args, **kwargs)\n",
    "        # Create the button.\n",
    "        self.description = description\n",
    "        self.disabled = disabled\n",
    "        #self.style = style\n",
    "        self.tooltip = tooltip\n",
    "        self.icon = icon\n",
    "        self.style.button_color = \"lightgreen\"\n",
    "        # Set on click behavior.\n",
    "        self.on_click(self.process)\n",
    "\n",
    "    def process(self, b):\n",
    "        cat, interest = CI.handle_input('upd')\n",
    "        view_update(cat, interest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CloseButton(widgets.Button):\n",
    "    \n",
    "    global CI\n",
    "    #global view_update\n",
    "    \n",
    "    def __init__(self, description='Close', disabled=False, style='', tooltip='Save and exit', icon='window-close', *args, **kwargs):\n",
    "        \"\"\"Initialize the LinkButton class.\"\"\"\n",
    "        super(CloseButton, self).__init__(*args, **kwargs)\n",
    "        # Create the button.\n",
    "        self.description = description\n",
    "        self.disabled = disabled\n",
    "        #self.style = style\n",
    "        self.tooltip = tooltip\n",
    "        self.icon = icon\n",
    "        self.style.button_color = \"lightgray\"\n",
    "        # Set on click behavior.\n",
    "        self.on_click(self.process)\n",
    "\n",
    "    def process(self, b):\n",
    "        is_saved = CI.handle_input('exit')\n",
    "        if not is_saved:\n",
    "            print('Could not save data & models')\n",
    "        clear_output()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShowSimilarButton(widgets.Button):\n",
    "    \n",
    "    global CI\n",
    "    global view_update\n",
    "    \n",
    "    def __init__(self, ID, description='Show similar', disabled=False, style='', tooltip='Find similar news', icon='search', *args, **kwargs):\n",
    "        \"\"\"Initialize the ShowSimilarButton class.\"\"\"\n",
    "        super(ShowSimilarButton, self).__init__(*args, **kwargs)\n",
    "        # Create the button.\n",
    "        self.ID = ID\n",
    "        self.description = description\n",
    "        self.disabled = disabled\n",
    "        #self.style = style\n",
    "        self.tooltip = tooltip\n",
    "        self.icon = icon\n",
    "        self.style.button_color = \"thistle\"\n",
    "        # Set on click behavior.\n",
    "        self.on_click(self.process)\n",
    "\n",
    "    def process(self, b):\n",
    "        similar = CI.handle_input('similar '+ self.ID)\n",
    "        view_similar(similar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "CI = Communicator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat, interest = CI.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jan 24 13:47:29 2020\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33c06f9328fa4f4ab0088214af60e874",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(UpdButton(description='Update', icon='refresh', style=ButtonStyle(button_color='lightgreen'), t…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1200764fd33741a8ac2f5874d43bce55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<br><h2>Interesting</h2>', placeholder='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e0bef8a856549c38e443ea8c7bb2fc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridBox(children=(HTML(value=\"<b>How Top Gear overcame its 'problem phase'</b>  <i>(Fri, 24 Jan 2020 00:52:09 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53132f6f219b44febabdec4cb625c003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<br><h2>BBC News - Home</h2>', placeholder='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5afe6a0b7bf3427d9a912bd847bd55d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridBox(children=(HTML(value='<b>China coronavirus: Death toll rises as more cities restrict travel</b>  <i>(F…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6160f2c1aa8408eb808fb835d5a4698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<br><h2>BBC News - World</h2>', placeholder='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31dbff63731640d88059522a00ae5229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridBox(children=(HTML(value='<b>China coronavirus: Death toll rises as more cities restrict travel</b>  <i>(F…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0c5c09e1d1343abac1e396452990100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<br><h2>BBC News - UK</h2>', placeholder='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1188094d8d4a47c3be0f4c9194a969d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridBox(children=(HTML(value='<b>Met Police to deploy facial recognition cameras</b>  <i>(Fri, 24 Jan 2020 12:…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ef7e3a327754f93a803c5bab94d427b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<br><h2>BBC News - Business</h2>', placeholder='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ef795a70c7245db97c846db5384fbcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridBox(children=(HTML(value='<b>UK firms see boost as uncertainty eases, survey says</b>  <i>(Fri, 24 Jan 202…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "949295a711574465959907b7ee6dcbc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<br><h2>BBC News - UK Politics</h2>', placeholder='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abe8b199f11b454ba7cc5f9149792301",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridBox(children=(HTML(value='<b>HS2 risks misjudged from the start, says watchdog</b>  <i>(Fri, 24 Jan 2020 0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3da062323e524bf789d8174fc8879552",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<br><h2>BBC News - Health</h2>', placeholder='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "754141ae64cf477895e4abe4f5d1a0a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridBox(children=(HTML(value='<b>East Kent hospitals: Care watchdog inspects trust after baby death apology</b…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c14e7ff4d57b4180802970516558d1db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<br><h2>BBC News - Family & Education</h2>', placeholder='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0557ef3d9f3b42e39708921475344c59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridBox(children=(HTML(value=\"<b>No time off for grieving: 'Inside I was screaming'</b>  <i>(Thu, 23 Jan 2020 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0af2292b43814363bca123f9a8ae03dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<br><h2>BBC News - Science & Environment</h2>', placeholder='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "694e48b93ac34e538f5540f29510513a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridBox(children=(HTML(value='<b>Space cookies: First food baked in space by astronauts</b>  <i>(Fri, 24 Jan 2…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f83090fcbecb4cb094408a2434702387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<br><h2>BBC News - Technology</h2>', placeholder='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07485bbe4ab34c88a786dd89ca2df1e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridBox(children=(HTML(value=\"<b>Facebook's Sir Nick Clegg criticised over WhatsApp security</b>  <i>(Fri, 24 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4ce4f31bb21454da906e9cae033891d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<br><h2>BBC News - Entertainment & Arts</h2>', placeholder='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "430067c8b1df4fd5944f1a36e3a81395",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridBox(children=(HTML(value=\"<b>How Top Gear overcame its 'problem phase'</b>  <i>(Fri, 24 Jan 2020 00:52:09 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "view_update(cat, interest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
